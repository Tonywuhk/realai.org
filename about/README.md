---
permalink: /about/
---
# About Real AI

Real AI is an [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) (AGI) research company based in Hong Kong. Unlike traditional companies, our [activities](#activities) are not profit-driven. We align ourselves with [effective altruism](http://realai.org/safety/effective-altruism/) and aim to benefit others as much as possible.

## Mission

Many of today’s [problems](https://80000hours.org/articles/cause-selection/), such as [poverty](https://80000hours.org/problem-profiles/health-in-poor-countries/), [disease](https://80000hours.org/problem-profiles/biosecurity/), and [climate change](https://80000hours.org/problem-profiles/climate-change/), can be solved by AGI developed to benefit the world. Real AI’s mission is to ensure that humanity has a bright future with [safe](http://realai.org/safety/) AGI. On a smart planet that automatically produces goods and services, everyone lives well and prosper. We do our best to turn this vision into reality.

## Activities

Real AI’s activities can be broadly divided into two areas: Strategy and Research.

### Strategy

Strategy sets Real AI’s mission, goals, and objectives, assesses them continuously, and seeks external validations of these assessments. To ensure Real AI’s activities are beneficial, we align ourselves with effective altruism and rely on research conducted by [80,000 Hours](https://80000hours.org/) to figure out how to [positively shape the development of artificial intelligence](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/). We also monitor the latest progress in [deep learning](https://en.wikipedia.org/wiki/Deep_learning) by studying the latest publications from select [research groups](http://realai.org/labs/), to ensure that our Research agenda is realistic. Our outreach [activities](http://realai.org/about/activities/) currently focus on educating the public about AI, with the hope that many people can benefit from a better understanding of what’s happening in a field that is poised to more significantly affect their lives in near future.

In addition to the above, Strategy maintains this website and handles non-productive overheads such as the [administration](http://realai.org/about/admin/) of Real AI Limited. We also opportunistically spend efforts to grow the AI safety community in China. 

### Research

Building safe and beneficial AI ultimately requires a technical solution. We conduct [technical research](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/#1-technical-research) in deep learning and take the [Prosaic AI](http://realai.org/prosaic/) view that its methods can scale to AGI without qualitatively new theories. As of July 2017, deep learning systems still lack the ability of [reasoning](http://realai.org/reasoning/), of which [automated theorem proving](http://realai.org/automated-theorem-proving/) (ATP) is a key aspect. This is the focus of our research.

We have a very concrete **technical goal**: use AI to produce a formal proof of a difficult math theorem in a simple computer language that allows highly reliable automated verifications of the proof’s correctness. One example of such a language is [Metamath](http://realai.org/metamath/). Based on our [subjective forecast as of August 2017](http://realai.org/forecasts/proving-mathematical-theorems/), unless delayed by [force majeure](http://realai.org/about/risks/), we believe there is a 50% chance that we’ll be able to build such an AI prover before the end of 2021, with a 90% confidence interval of plus or minus 12 months. This estimate assumes that we'll be 5 months behind the relevant research [frontiers](http://realai.org/frontiers/) in 2021. We assign a chance of less than 5% that we’ll be the first research group to reach this milestone.

A theorem prover is a domain-specific [oracle](http://realai.org/safety/oracle-ai/), a term commonly used in the AI safety community. Superintelligent oracles might possess the ability to manipulate its human users. Within the scope of our Research, we believe that even a highly intelligent AI theorem prover will be safe as it only processes logical symbols and knows very little about the real world, but we have not conducted a rigorous safety review and welcome any feedback. We hope that knowledge in AI reasoning can stimulate more effective research in safety, and that an advanced ATP system can help [Machine Intelligence Research Institute](https://intelligence.org/) in its quest of “foundational mathematical research” that ensures AGI has a positive impact.

#### Longer-Term Contingencies

A mathematical proof is essentially a piece of code written in a programming language of very limited expressiveness. As the scope of the language expands, the natural next steps are a code generation system in a high-level programming language, then a common sense reasoner in a natural language. The last step lays a solid foundation upon which to build an AGI chatbot. When we notice any of the milestones is achieved, we will shift our technical goal to the next step. Each of these three steps after ATP seems to be worth around two years of solid AI progress worldwide, putting our AGI target at 2027. Along this path, once we pass the stage of purely mathematical reasoning, the AI systems under Research will inevitably interact more with the real world. Under any circumstances, we will ensure that sufficiently robust safety measures are in place.

## Team

[Jonathan Yan](https://www.linkedin.com/in/jonathan-yan-766461130/). Founder and Director.

## About This Site

The field of AI is experiencing explosive growth and important new information is being produced almost every day. This site is a place where we put the relevant information together. We hope to show the big picture more clearly, to make scientific progress more accessible, and to accelerate the development of safe and beneficial AGI. Contents on this site are **not meant to be complete** and are frequently updated.

This site is hosted on [GitHub Pages](https://pages.github.com/), based on the [Slate](https://github.com/pages-themes/slate) theme, and generated using [Jekyll](http://jekyllrb.com/).

