---
permalink: /generative-models/
---
# Generative Models

Generative models are likely what humans and some other animals rely on for [planning](http://realai.org/planning/).

## Variational Autoencoders

The variational autoencoders (VAE) framework combines a neural-network based inference model with a neural-network based generative model. For an overview, see

* 2017 [September 30](https://twitter.com/dpkingma/status/914277278602821633), Diederik P. Kingma. [Variational Inference & Deep Learning: A New Synthesis](https://www.dropbox.com/s/v6ua3d9yt44vgb3/cover_and_thesis.pdf) (PDF). *Dropbox*.

A subset of latent variables in a VAE can be made interpretable by applying semi-supervised learning, where labels are supplied for these variables ([Siddharth et al., 2017](https://arxiv.org/abs/1706.00400)).

### References

* 2017 June 1, N. Siddharth, Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Frank Wood, Noah D. Goodman, Pushmeet Kohli, and Philip H. S. Torr. [Learning Disentangled Representations with Semi-Supervised Deep Generative Models](https://arxiv.org/abs/1706.00400). *arXiv:1706.00400*.

## Generative Adversarial Networks

[Hu et al. (2017)](https://arxiv.org/abs/1706.00550) establish formal connections between GANs and VAEs, and note that GANs are also related to reinforcement learning, teacher-student distillation and wake-sleep algorithm.

### References

* 2017 June 2, Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric P. Xing. [On Unifying Deep Generative Models](https://arxiv.org/abs/1706.00550). *arXiv:1706.00550*.

## Memory-Augmented Generative Models

* 2017 September 21, JÃ¶rg Bornschein, Andriy Mnih, Daniel Zoran, and Danilo J. Rezende. [Variational Memory Addressing in Generative Models](https://arxiv.org/abs/1709.07116). *arXiv:1709.07116*.

