---
permalink: /progress/
---
# Progress

The basic idea of using artificial neural networks to build human-level intelligence has been around for a long time. For decades, it was considered a dead end despite its intellectual appeal and soundness because there was little evidence that neural networks could solve practical problems better than other methods. It turned out that the idea was foundamentally correct, it was mainly due to insufficient computing power that the promise of neural networks didn't materialize.

Since 2012, there have been rapid advances in various fronts towards the technology to build general intelligence. A common theme in these advances is that the basic ideas were discovered decades ago, long before it was practical to test them out in large scale. With enough computing power, natural extensions of those ideas demonstrated surprising successes in many cognitive tasks. At the RSA Conference on 23 February 2017, Alphabet chairman Eric Schmidt [said in an interview](https://youtu.be/-UViiNq-dxk?t=6m) that both he and Alphabet co-founder Larry Page now believe that simple algorithms can scale to reproduce human abilities.

DeepMind [told the Financial Times](https://www.ft.com/content/cada14c4-d366-11e6-b06b-680c49b4b4c0) on 17 March 2017 that it hoped to publish six more papers in highly regarded scientific journals such as Nature or Science within the next year. We look forward to the addition of substantial new progress to this page in the coming months.

* Community effort: [Measuring the Progress of AI Research](https://www.eff.org/ai/metrics). *Electronic Frontier Foundation*.

## Deep Neural Networks

* 2015 February 11, Sergey Ioffe and Christian Szegedy. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167). *arXiv:1502.03167*.
* 2014 December 22, Diederik P. Kingma and Jimmy Ba. [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). *arXiv:1412.6980*.
* 2012 July 3, Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580). *arXiv:1207.0580*.

## Recurrent Neural Networks

* 2014 September 10, Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215). *arXiv:1409.3215*.
* 2013 August 4, Alex Graves. [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850). *arXiv:1308.0850*.
* 1997 November 15, Sepp Hochreiter and Jürgen Schmidhuber. [Long Short-Term Memory](http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735). *Neural Computation*, 9:1735-1780.

## Deep Learning for Recognition

* 2012 December 6, Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks). [*NIPS 2012*](https://nips.cc/Conferences/2012/Schedule?type=Poster).
* 2012 October 13. SuperVision won [ILSVRC2012](http://www.image-net.org/challenges/LSVRC/2012/). [*ImageNet*](http://image-net.org/challenges/LSVRC/2012/results.html).

By June 2017, the training of a ResNet-50 model on ImageNet [took just one hour](https://research.fb.com/publications/ImageNet1kIn1h/), using a minibatch size of 8192 on 256 GPUs.

## Model-Free Deep Reinforcement Learning

* 2016 February 4, Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783). *arXiv:1602.01783*.
* 2013 December 19, Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602). *arXiv:1312.5602*.

## Attention and External Memory

* 2016 May 19, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). *arXiv:1409.0473*.
* 2014 October 20, Alex Graves, Greg Wayne, and Ivo Danihelka. [Neural Turing Machines](https://arxiv.org/abs/1410.5401). *arXiv:1410.5401*.
* 2014 October 15, Jason Weston, Sumit Chopra, and Antoine Bordes. [Memory Networks](https://arxiv.org/abs/1410.3916). *arXiv:1410.3916*.

## Generative Models

* 2014 June 10, Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661). *arXiv:1406.2661*.
* 2013 December 20, Diederik P Kingma and Max Welling. [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114). *arXiv:1312.6114*.

## Unsupervised Learning

In June 2012, Google [discovered](https://googleblog.blogspot.hk/2012/06/using-large-scale-brain-simulations-for.html) that one artificial neuron of its large-scale neural network learned to respond to pictures of cats ([Le et al., 2012](https://arxiv.org/abs/1112.6209)), despite the fact that no supervisory signals were given during training. Their models had more than 1 billion connections, an order of magnitude larger than other large networks used back then.

## Language Grounding

* 2013 January 16, Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781). *arXiv:1301.3781*.

## Significant Demonstrations

### AlphaGo

* 2016 March 15. AlphaGo defeated [Lee Sedol](https://en.wikipedia.org/wiki/Lee_Sedol) 4-1. [*The Guardian*](https://www.theguardian.com/technology/2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee-sedol), [*Wikipedia*](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol).
* 2015 November 11, David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. [Mastering the game of Go with deep neural networks and tree search](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html). *Nature*, 529(7587):484-489.

### Poker

* 2017 May 5, Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. [DeepStack: Expert-level artificial intelligence in heads-up no-limit poker](http://science.sciencemag.org/content/356/6337/508). *Science*, 356(6337):508-513.

### Machine Translation

* 2016 September 27. Google uses a neural network for machine translation. [*Wired*](https://www.wired.com/2016/09/google-claims-ai-breakthrough-machine-translation/), [*Google Research Blog*](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html).
* 2016 September 26, Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144). *arXiv:1609.08144*.

## Highly Cited Works

* 2015 August 26, Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.6576). *arXiv:1508.6576*.
* 2006 July 28, G. E. Hinton and R. R. Salakhutdinov. [Reducing the Dimensionality of Data with Neural Networks](http://science.sciencemag.org/content/313/5786/504). *Science*, 313(5786):504-507.
* 2006 May 17, Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. [A Fast Learning Algorithm for Deep Belief Nets](http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527). *Neural Computation*, 18:1527-1554.

## References

* 2012 July 12, Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and Andrew Y. Ng. [Building high-level features using large scale unsupervised learning](https://arxiv.org/abs/1112.6209). *arXiv:1112.6209*.
