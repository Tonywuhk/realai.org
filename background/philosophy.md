---
permalink: /background/philosophy/
mathjax: true
---
# Philosophy

## What Is Intelligence?

Many systems operating inside an environment exhibit a certain degree of complexity. Intelligence is such complexity as perceived by us. This definition is first of all subjective, and our subjective views can differ greatly. Take the weather as an example, in ancient times, people resorted to myths for an explanation, implicitly assuming intelligence. In 2003, [Stephen Wolfram](https://en.wikipedia.org/wiki/Stephen_Wolfram) [quoted](https://web.stanford.edu/dept/news/pr/03/wolfram129.html) the aphorism “the weather has a mind of its own,” and [explained in a 2017 blog](http://blog.stephenwolfram.com/2017/05/a-new-kind-of-science-a-15-year-view/) that its fluid dynamics is the same in computational sophistication as the electrical processes in human brains, a [point](http://www.wolframscience.com/nks/p844--historical-perspectives/) made in his book “[A New Kind of Science](https://www.wolframscience.com/).” However, most people today clearly do not consider the weather intelligent, largely because we’ve already had a reasonably good understanding of atmospheric sciences. Other examples include:

* AlphaGo ([Silver & Huang et al., 2016](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)). It would be considered AI a few years before its development, but not so once the paper explaining its principles are published, as the readers can easily build a mental model of how it works.
* iPhone. Again it would be considered AI shortly around the years mobile phones were invented, but not so by the time it was in mass production.

It is safe to expect that humans will always be considered intelligent, but no lesser systems appear safe as the capability of machine intelligence steadily improves. Animal intelligence can also take vastly different forms from the familiar nervous system centered in the human brain. According to [an article on Quanta Magazine](https://www.quantamagazine.org/the-thoughts-of-a-spiderweb-20170523/), two-thirds of the neurons in an octopus are found in its arms, and spiders appear to solve cognitive tasks using their webs!

The scope of an AI system ranges from classical reinforcement learning agents that are individualistic with a stable boundary, to a distributed population of physical processes with a certain level of coordination. The latter end of the scope is explained more rigorously in [Weinbaum & Veitas (2015)](https://arxiv.org/abs/1505.06366). A distributed intelligence emerging from the Internet is called the [global brain](https://en.wikipedia.org/wiki/Global_brain) and is studied by [The Global Brain Institute](http://globalbraininstitute.org). The difference between open-ended and utility-based intelligence is discussed in [Goertzel (2015)](http://jetpress.org/v25.2/goertzel.htm), particularly its Section 9. Under this very broad interpretation, many social sites can be considered crowd computers. Twitter allows its users to assemble feeds from other users. It transmits information fast and very efficiently due to the 140-character limit. Reddit organizes information by subjects (subreddits) that have more depth but less flexibility. Facebook allows a wider range of data from its users. If we think of Facebook like a mini-Internet, then Twitter will be a nano-Internet. Under this very unconventional view of intelligence, it can be said that a distributed intelligence is emerging from the Internet. If our interconnnected world has reached singularity, then we should try [Positively Shaping the Nature of Our Global Superintelligence (draft)](http://realai.org/blog/positively-shaping-the-nature-of-our-global-superintelligence/).

### References

* 2016 January 28, David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. [Mastering the game of Go with deep neural networks and tree search](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html). *Nature*, 529(7587):484-489.
* 2015 November. Ben Goertzel. [Superintelligence: Fears, Promises and Potentials](http://jetpress.org/v25.2/goertzel.htm). *Journal of Evolution & Technology*, 24(2):55-87.
* 2015 June 12, David Weinbaum and Viktoras Veitas. [Open Ended Intelligence: The individuation of Intelligent Agents](https://arxiv.org/abs/1505.06366). *arXiv:1505.06366*.

## Consciousness

[Friston (2017)](https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference) argues that consciousness is a process about inferring the causes of sensory states, and thereby navigating the world to elude surprises. We invoke this concept to describe a system when its internal model of self and the world has sufficient *counterfactural depth*, the temporal distance to the future on which the system can grasp the impact of its actions. Curiosity is not mentioned in this essay, and it is not clear how the curious behavior of seeking surprises can be reconciled with a disinterested mind. But it is conceivable that meaningful curiosity can be interpreted as the system acquiring information to reduce future uncertainty.

[Lawrence (2017)](https://arxiv.org/abs/1705.07996) suggests that we develop complex cognitive processes including consciousness as we make best use of the limited bandwidth for [communication](http://realai.org/multi-agent-learning/#communication). The *embodiment factor*, the ratio of compute power to communication bandwidth, is a distinguishing feature of human and current machine intelligence. A typical desktop computer today has an embodiment factor of around \\(10\\), while the huge processing power of human brain and the very low information rate of reading or talking yield an embodiment factor of around \\(10^{16}\\).

[Kang et al. (2017)](http://www.cell.com/current-biology/fulltext/S0960-9822(17)30784-4) provide experimental evidence that conscious awareness of having reached a decision arises when the brain’s representation of accumulated evidence reaches a threshold or bound.

[Integrated information theory](https://en.wikipedia.org/wiki/Integrated_information_theory) is a proposed fundamental theory of consciousness that, according to a Wired [report](https://www.wired.com/2013/11/christof-koch-panpsychism-consciousness/) in 2013, assigns to any complex system a number of how integrated it is, as a measure of consciousness.

The attention schema theory (AST; [Webb & Graziano, 2015](http://journal.frontiersin.org/article/10.3389/fpsyg.2015.00500/full)) explains consciousness using the mechanics of [attention](http://realai.org/attention/). In an [introductory article](https://www.theatlantic.com/science/archive/2016/06/how-consciousness-evolved/485558/) published in June 2016, Graziano explained the development of mechanism capable of self-modelling in three stages: selective signal enhancement, overt attention, and covert attention. AST started as a model of one’s own covert attention and enabled the brain to attribute consciousness to itself.

### Implications

Theories of consciousness can inspire algorithmic choices in deep learning. [Bengio (2017)](https://arxiv.org/abs/1709.08568) defines the *conscious state* as a very low-dimensional vector derived from the representation state of an RNN by a form of attention mechanism.

### References

* 2017 September 25, Yoshua Bengio. [The Consciousness Prior](https://arxiv.org/abs/1709.08568). *arXiv:1709.08568*.
* 2017 July 27, Yul H.R. Kang, Frederike H. Petzschner, Daniel M. Wolpert, and Michael N. Shadlen. [Piercing of Consciousness as a Threshold-Crossing Operation](http://www.cell.com/current-biology/fulltext/S0960-9822(17)30784-4). *Current Biology*.
* 2017 May 22, Neil D. Lawrence. [Living Together: Mind and Machine Intelligence](https://arxiv.org/abs/1705.07996). *arXiv:1705.07996*.
* 2017 May 18, Karl Friston. [The mathematics of mind-time](https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference). *Aeon*.
* 2015 April 23, Taylor W. Webb and Michael S. A. Graziano. [The attention schema theory: a mechanistic account of subjective awareness](http://journal.frontiersin.org/article/10.3389/fpsyg.2015.00500/full). *Front. Psychol.*

## Utilities

The word utility has been defined in various ways. In the philosophical context of utilitarianism, it is about the well-being of sentient entities. As an ethical theory, the core concept of utilitarianism is that we should "bring about a world in which every individual has the highest possible level of well-being" ([de Lazari-Radek & Singer, 2017](https://global.oup.com/academic/product/utilitarianism-a-very-short-introduction-9780198728795)). Implicitly assumed here, is that the utility of the *world* is in a function defined on individual utilities:

$$
U = F(u_0, u_1, ..., u_N),
$$

where \\(U\\) is the utility of the world, and the \\(u_i\\)'s are the utilities of properly defined individuals. This is a strong assumption and severely restricts the space of goals for agents developed under the principles in deep [reinforcement learning](http://realai.org/course/reinforcement-learning/), which typically includes the optimization of a target that can be framed as a form of "utility." We plan to rigorously dispute this view. Our current thinking is [being developed as a blog post](http://realai.org/blog/goals-and-utilities/).

### References

* 2017 July 27, Katarzyna de Lazari-Radek and Peter Singer. [Utilitarianism: A Very Short Introduction](https://global.oup.com/academic/product/utilitarianism-a-very-short-introduction-9780198728795). *Oxford University Press*.

## Creativity

Not only machine learning-based AI systems can have creativity, it is also likely prevalent in environments where learned policies are much closer to optimal than human policies. When AI acts in a way that is more optimal than normal human policies, it can appear creative from human’s perspective. In a modern example, an AI system AlphaGo learned to play the ancient Chinese game of Go using tree-based planning with machine intuition learned from a large number of self-played games. From AlphaGo’s perspective, its Go moves are the natural results of its algorithm. Some of the moves will be surprising for the intuitive part of AlphaGo, if tree search returns a solution that is previously considered a poor move based only on machine intuition. When AlphaGo makes a winning move that is difficult for humans to think of, but interpretable after it is played, such moves are often considered creative by expert human players, simply because human players interpret Go moves differently.

In rule-based AI systems, creativity is rarely seen because by design, policies are explicitly programmed into the machine by human programmers. A machine cannot come up with anything that its programmers don’t understand, because there is no code in the machine that allows it to learn directly from data. Since most of today’s AI systems are rule-based, it is a common belief that creativity is a hard problem in AI.

## Emergence

Complex macroscopic patterns can emerge from a collection of simple processes. In [this video](https://www.youtube.com/watch?v=pNe6fsaCVtI), a rolling circle emerges from multiple well-positioned dots moving in a straight line. Intelligence doesn’t exist among the atoms, the reductionist conclusion is that intelligence doesn’t exist at all, which is clearly absurd. Even neurons or combination of a few neurons follow very simple, non-intelligence rules, it is entirely plausible that complex intelligence can arise when they work together. There is [a theory of reality](https://www.quantamagazine.org/a-theory-of-reality-as-more-than-the-sum-of-its-parts-20170601/) in which conscious beings might have greater influence over the future than does the sum of their microscopic components

## Anthropic Reasoning

The [Self-Sampling Assumption](http://www.anthropic-principle.com/?q=book/chapter_3#3d) (SSA) is the assumption that "one should reason as if one were a random sample from the set of all observers in one's reference class." It leads to various types of [Doomsday Arguments](http://www.anthropic-principle.com/?q=book/chapter_6) that predict fairly short life-expectancy of the human species.

[Pereira (2017)](https://arxiv.org/abs/1705.03078) introduces the Super-Strong Self-Sampling Assumption (SSSSA) that human consciousness should be a typical sample in the consciousness-space-time. He then concludes that superintelligent AI would probably not be created because otherwise it would dominate the consciousness-space-time.

### References

* 2017 May 8, Toby Pereira. [An Anthropic Argument against the Future Existence of Superintelligent Artificial Intelligence](https://arxiv.org/abs/1705.03078). *arXiv:1705.03078*.

