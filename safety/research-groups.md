---
permalink: /safety/research-groups/
---
# AI Safety Research Groups

Many research groups that work on AI safety are connected with [effective altruism](http://realai.org/safety/effective-altruism/) (EA).

## Machine Intelligence Research Institute

The [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI) is a nonprofit that does foundational mathematical research to ensure smarter-than-human AI has a positive impact. Its [mission](https://intelligence.org/about/) is to develop formal tools with the intent of making AI systems safer and more reliable. Based on its [background claims](https://intelligence.org/2015/07/24/four-background-claims/) that  advances in AI will be among the largest determiners of human welfare, MIRI [identifies with EA](https://intelligence.org/2015/08/28/ai-and-effective-altruism/).

MIRI has a [team](https://intelligence.org/team/) of staff, advisors, research associates, and board members. All its [publications](https://intelligence.org/all-publications/) are available online. 

### History

MIRI was formerly known as the Singularity Institute (SI) for Artificial Intelligence. SI was founded in 2000 by [Eliezer S. Yudkowsky](http://yudkowsky.net/) and initially funded by Brian and Sabine Atkins to "accelerate toward AI" ([FLI, 2015](https://futureoflife.org/2015/10/11/113/)). [Luke Muehlhauser](http://lukemuehlhauser.com/) became SI's Executive Director in 2011.

Until at least 11 May 2012, it had not been a charity recommended by GiveWell Labs (presently known as the [Open Philanthropy Project](http://www.openphilanthropy.org/)). GiveWell's [Holden Karnofsky](http://www.openphilanthropy.org/about/team/holden-karnofsky) detailed his thoughts on why SI was not recommended in a [post](http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/) on Less Wrong's [discussion board](http://lesswrong.com/r/discussion/new/).

SI agreed to change name following Singularity University's [acquisition](http://singularityu.org/2012/12/09/singularity-university-acquires-the-singularity-summit/) of the Singularity Summit in December 2012, and [became](https://intelligence.org/2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri/) MIRI the following January.

Luke [left](https://intelligence.org/2015/05/06/a-fond-farewell-and-a-new-executive-director/) on 6 May 2015 to accept a research postion at [GiveWell](http://realai.org/safety/effective-altruism/#givewell-good-ventures-and-the-open-philanthropy-project). [Nate Soares](http://so8r.es/) has been MIRI's Executive Director since then.

In its [2017 Updates and Strategy](https://intelligence.org/2017/04/30/2017-updates-and-strategy/) published on 30 April 2017, MIRI researchers adjusted upwards their estimated probability of AGI’s being developed before 2035. Consequently, MIRI expected AGI would bear closer resemblance to present-day AI systems. It started [hiring software engineers](https://intelligence.org/2017/04/30/software-engineer-internship-staff-openings/).

## Center for Applied Rationality

[Center for Applied Rationality](http://rationality.org/) (CFAR) is a non-profit 501(c)(3) tax-exempt organization (EIN 45-3100226) in Berkeley, California. Rationality here means [“actually trying to figure things out.”](http://rationality.org/about/mission) It runs 4-day immersive workshops priced at [USD 3,900](http://rationality.org/workshops/faq#what-is-the-price-of-the-workshop) that might [not be a good fit](http://rationality.org/workshops/faq#who-shouldnt-attend-cfar-workshops) for people uncomfortable with extensive socialization. All their workshops [aim narrowly](http://rationality.org/about/mission#third-we-are-focused-specifically-on-existential-win-and-on-the-people-social-fabric-and-thinking-skills-that-might-most-help-with-that--we-see-ai-safety-as-especially-key-here) at: (a) AI safety directly; or (b) developing our rationality. [Participants below 18 years old are not allowed](http://rationality.org/workshops/faq#im-not-18-yet-can-i-still-attend) at their core workshops.

CFAR was spun off from MIRI in 2012, with which it still shares an office. Its co-founders Anna Salamon (President & Chair of Board), Julia Galef (President), Michael "Valentine" Smith (Research), and Andrew Critch (Curriculum Developer) are on the current [team](http://rationality.org/about/team) of staff, contractors, adjunct instructors, staff alumni, and board of directors.

## [Future of Humanity Institute](https://www.fhi.ox.ac.uk/):

* [Publications](http://www.fhi.ox.ac.uk/publications/)
* [Team](https://www.fhi.ox.ac.uk/about/the-team/)

## [Future of Life Institute](https://futureoflife.org/)

## [UC Berkeley Center for Human-Compatible AI](http://humancompatible.ai/)

## [Center for the Study of Existential Risk](http://cser.org/)

## Leverhulme Center for the Future of Intelligence

The [Leverhulme Center for the Future of Intelligence](http://lcfi.ac.uk/) was [launched](http://www.cam.ac.uk/research/news/the-future-of-intelligence-cambridge-university-launches-new-centre-to-study-ai-and-the-future-of) in December 2015 by the University of Cambridge, where it is also based at. It is funded by a £10 million grant from the [Leverhulme Trust](https://www.leverhulme.ac.uk/), and has a clear practical goal, "to work together to ensure that we humans make the best of the opportunities of artificial intelligence as it develops over coming decades." It runs a series of [projects](http://www.lcfi.ac.uk/projects/) on the nature and impact of AI. [Demis Hassabis](http://lcfi.ac.uk/about/people/demis-hassabis/), co-founder and CEO of DeepMind, currently sits on CFI's [international advisory board](http://lcfi.ac.uk/about/people/).

## [Foundational Research Institute](https://foundational-research.org/)

## References

* 2016 December 13, Ben Henry. [2017 AI Risk Literature Review and Charity Comparison](http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/). *Effective Altruism Forum*.
* 2015 October 11, Future of Life Institute (FLI). [MIRI: Artificial Intelligence: The Danger of Good Intentions](https://futureoflife.org/2015/10/11/113/). *Future of Life Institute*.
