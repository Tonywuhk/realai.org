---
permalink: /philosophy/
mathjax: true
---
# Philosophy

[Pereira (2017)](https://arxiv.org/abs/1705.03078) introduces the Super-Strong Self-Sampling Assumption (SSSSA) that human consciousness should be a typical sample in the consciousness-space-time. He then concludes that superintelligent AI would probably not be created because otherwise it would dominate the consciousness-space-time.

## What Is Intelligence?

Many systems operating inside an environment exhibit a certain degree of complexity. Intelligence is such complexity as perceived by us. This definition is first of all subjective. Take the atmosphere for example, it is not considered intelligent today as we have already had a reasonably good understanding of atmospheric sciences. In ancient times, however, people resorted to myths for an explanation, implicitly assuming intelligence. Other examples include:

* AlphaGo ([Silver & Huang et al., 2016](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)). It would be considered AI a few years before its development, but not so once the paper explaining its principles are published, as the readers can easily build a mental model of how it works.
* iPhone. Again it would be considered AI shortly around the years mobile phones were invented, but not so by the time it was in mass production.

It is safe to expect that humans will always be considered intelligent, but no lesser systems appear safe as the capability of machine intelligence steadily improves. Animal intelligence can also take vastly different forms from the familiar nervous system centered in the human brain. According to [an article on Quanta Magazine](https://www.quantamagazine.org/the-thoughts-of-a-spiderweb-20170523/), two-thirds of the neurons in an octopus are found in its arms, and spiders appear to solve cognitive tasks using their webs!

The scope of an AI system ranges from classical reinforcement learning agents that are individualistic with a stable boundary, to a distributed population of physical processes with a certain level of coordination. The latter end of the scope is explained more rigorously in [Weinbaum & Veitas (2015)](https://arxiv.org/abs/1505.06366). A distributed intelligence emerging from the Internet is called the [global brain](https://en.wikipedia.org/wiki/Global_brain) and is studied by [The Global Brain Institute](http://globalbraininstitute.org). The difference between open-ended and utility-based intelligence is discussed in [Goertzel (2015)](http://jetpress.org/v25.2/goertzel.htm), particularly its Section 9.

## Consciousness

[Friston (2017)](https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference) argues that consciousness is a process about inferring the causes of sensory states, and thereby navigating the world to elude surprises. We invoke this concept to describe a system when its internal model of self and the world has sufficient *counterfactural depth*, the temporal distance to the future on which the system can grasp the impact of its actions. Curiosity is not mentioned in this essay, and it is not clear how the curious behavior of seeking surprises can be reconciled with a disinterested mind. But it is conceivable that meaningful curiosity can be interpreted as the system acquiring information to reduce future uncertainty.

[Lawrence (2017)](https://arxiv.org/abs/1705.07996) suggests that we develop complex cognitive processes including consciousness as we make best use of the limited bandwidth for [communication](http://realai.org/multi-agent-learning/#communication). The *embodiment factor*, the ratio of compute power to communication bandwidth, is a distinguishing feature of human and current machine intelligence. A typical desktop computer today has an embodiment factor of around \\(10\\), while the huge processing power of human brain and the very low information rate of reading or talking yield an embodiment factor of around \\(10^{16}\\).

## Creativity

Not only machine learning-based AI systems can have creativity, it is also likely prevalent in environments where learned policies are much closer to optimal than human policies. When AI acts in a way that is more optimal than normal human policies, it can appear creative from human’s perspective. In a modern example, an AI system AlphaGo learned to play the ancient Chinese game of Go using tree-based planning with machine intuition learned from a large number of self-played games. From AlphaGo’s perspective, its Go moves are the natural results of its algorithm. Some of the moves will be surprising for the intuitive part of AlphaGo, if tree search returns a solution that is previously considered a poor move based only on machine intuition. When AlphaGo makes a winning move that is difficult for humans to think of, but interpretable after it is played, such moves are often considered creative by expert human players, simply because human players interpret Go moves differently.

In rule-based AI systems, creativity is rarely seen because by design, policies are explicitly programmed into the machine by human programmers. A machine cannot come up with anything that its programmers don’t understand, because there is no code in the machine that allows it to learn directly from data. Since most of today’s AI systems are rule-based, it is a common belief that creativity is a hard problem in AI.

## Emergence

Complex macroscopic patterns can emerge from a collection of simple processes. In [this video](https://www.youtube.com/watch?v=pNe6fsaCVtI), a rolling circle emerges from multiple well-positioned dots moving in a straight line. Intelligence doesn’t exist among the atoms, the reductionist conclusion is that intelligence doesn’t exist at all, which is clearly absurd. Even neurons or combination of a few neurons follow very simple, non-intelligence rules, it is entirely plausible that complex intelligence can arise when they work together. There is [a theory of reality](https://www.quantamagazine.org/a-theory-of-reality-as-more-than-the-sum-of-its-parts-20170601/) in which conscious beings might have greater influence over the future than does the sum of their microscopic components

## References

* 2017 May 22, Neil D. Lawrence. [Living Together: Mind and Machine Intelligence](https://arxiv.org/abs/1705.07996). *arXiv:1705.07996*.
* 2017 May 18, Karl Friston. [The mathematics of mind-time](https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference). *Aeon*.
* 2017 May 8, Toby Pereira. [An Anthropic Argument against the Future Existence of Superintelligent Artificial Intelligence](https://arxiv.org/abs/1705.03078). *arXiv:1705.03078*.
* 2016 January 28, David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. [Mastering the game of Go with deep neural networks and tree search](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html). *Nature*, 529(7587):484-489.
* 2015 November. Ben Goertzel. [Superintelligence: Fears, Promises and Potentials](http://jetpress.org/v25.2/goertzel.htm). *Journal of Evolution & Technology*, 24(2):55-87.
* 2015 June 12, David Weinbaum and Viktoras Veitas. [Open Ended Intelligence: The individuation of Intelligent Agents](https://arxiv.org/abs/1505.06366). *arXiv:1505.06366*.
